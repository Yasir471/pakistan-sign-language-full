# ğŸ¤Ÿ Pakistani Sign Language Detection with YOLOv5

Real-time Pakistani Sign Language detection and translation system using YOLOv5, supporting **Urdu**, **Pashto**, and **English** languages.

## ğŸ¯ Features

- ğŸ§  **YOLOv5-powered** real-time hand gesture detection
- ğŸ‡µğŸ‡° **Pakistani Sign Language** support (20+ gestures)
- ğŸ”„ **Bidirectional conversion**: Sign â†” Speech
- ğŸ¤ **Speech recognition** in Urdu, Pashto, and English  
- ğŸ”Š **Text-to-speech** output for detected gestures
- ğŸ“¹ **Live camera feed** processing
- ğŸ¤š **Real hand tracking** and gesture classification

## ğŸ“ Project Structure

```
sign_app/
â”œâ”€â”€ best.pt                 # Trained YOLOv5 model
â”œâ”€â”€ sign_to_speech.py      # Real-time gesture â†’ speech
â”œâ”€â”€ speech_to_sign.py      # Speech â†’ gesture display
â”œâ”€â”€ labels.json            # Gesture labels (Urdu/Pashto/English)
â”œâ”€â”€ train_yolov5_colab.py  # Training script for Google Colab
â”œâ”€â”€ gesture_images/        # Sample gesture images
â”‚   â”œâ”€â”€ salam.jpg
â”‚   â”œâ”€â”€ shukriya.jpg
â”‚   â””â”€â”€ ...
â””â”€â”€ README.md             # This file
```

## ğŸš€ Quick Start

### Prerequisites

```bash
# Install Python dependencies
pip install opencv-python torch torchvision ultralytics
pip install speechrecognition pyttsx3 pillow numpy
pip install pyaudio  # For microphone input
```

### 1. ğŸ“¹ Sign to Speech Detection

Detects hand gestures from camera and converts to speech:

```bash
python sign_to_speech.py
```

**What it does:**
- Opens camera feed
- Detects Pakistani hand gestures using YOLOv5
- Speaks the detected gesture in English/Urdu/Pashto
- Shows real-time bounding boxes and confidence scores

### 2. ğŸ¤ Speech to Sign Conversion  

Converts spoken words to gesture demonstrations:

```bash
python speech_to_sign.py
```

**What it does:**
- Listens to microphone input
- Recognizes speech in Urdu/Pashto/English
- Shows corresponding hand gesture image
- Displays gesture information in all three languages

### 3. ğŸ‹ï¸ Training Your Own Model

Use Google Colab to train custom YOLOv5 model:

1. Upload `train_yolov5_colab.py` to Google Colab
2. Add your gesture images to dataset folders
3. Run training script
4. Download trained `best.pt` model

## ğŸ“Š Supported Gestures

| Gesture | Urdu | Pashto | English |
|---------|------|--------|---------|
| salam | Ø³Ù„Ø§Ù… | Ø³Ù„Ø§Ù… ÙˆØ±ÙˆØ± | Hello |
| shukriya | Ø´Ú©Ø±ÛŒÛ | Ù…Ù†Ù†Ù‡ | Thank you |
| khuda_hafiz | Ø®Ø¯Ø§ Ø­Ø§ÙØ¸ | Ø®Ø¯Ø§ÛŒ Ù¾Ø§Ù…Ø§Ù† | Goodbye |
| paani | Ù¾Ø§Ù†ÛŒ | Ø§ÙˆØ¨Ù‡ | Water |
| khana | Ú©Ú¾Ø§Ù†Ø§ | Ø®ÙˆØ§Ú“Ù‡ | Food |
| madad | Ù…Ø¯Ø¯ | Ù…Ø±Ø³ØªÙ‡ | Help |
| ek | Ø§ÛŒÚ© | ÛŒÙˆ | One |
| do | Ø¯Ùˆ | Ø¯ÙˆÙ‡ | Two |
| teen | ØªÛŒÙ† | Ø¯Ø±Û | Three |
| ghar | Ú¯Ú¾Ø± | Ú©ÙˆØ± | Home |
| ... | ... | ... | ... |

*20+ gestures supported (see `labels.json` for complete list)*

## ğŸ› ï¸ How It Works

### Sign Detection Pipeline
1. **Camera Input** â†’ Capture live video feed
2. **YOLOv5 Processing** â†’ Detect hand gestures in real-time
3. **Classification** â†’ Identify specific Pakistani gestures
4. **Translation** â†’ Convert to Urdu/Pashto/English text
5. **Speech Output** â†’ Text-to-speech announcement

### Speech Recognition Pipeline  
1. **Microphone Input** â†’ Capture spoken words
2. **Speech Recognition** â†’ Convert speech to text (multi-language)
3. **Gesture Matching** â†’ Find corresponding hand gesture
4. **Visual Display** â†’ Show gesture image and information
5. **Audio Feedback** â†’ Confirm gesture found

## ğŸ“ Training Your Own Model

### Step 1: Prepare Dataset

```bash
# Create dataset structure
pakistani_sign_dataset/
â”œâ”€â”€ train/
â”‚   â”œâ”€â”€ images/        # Training images
â”‚   â””â”€â”€ labels/        # YOLO format labels
â””â”€â”€ val/
    â”œâ”€â”€ images/        # Validation images  
    â””â”€â”€ labels/        # YOLO format labels
```

### Step 2: Label Format

YOLO format labels (one `.txt` file per image):
```
class_id x_center y_center width height
0 0.5 0.3 0.4 0.6
```

### Step 3: Train in Google Colab

```python
# Upload train_yolov5_colab.py to Colab
# Run training script
python train_yolov5_colab.py
```

### Step 4: Download Model

After training, download `best.pt` and replace the existing model.

## ğŸ”§ Configuration

### Camera Settings
```python
# In sign_to_speech.py
cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)
cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)
cap.set(cv2.CAP_PROP_FPS, 30)
```

### Detection Thresholds
```python
# In sign_to_speech.py
self.model.conf = 0.6  # Confidence threshold
self.model.iou = 0.4   # IoU threshold
```

### Speech Recognition Languages
```python
# In speech_to_sign.py
# Supported languages: 'en', 'ur', 'auto'
text = recognizer.recognize_google(audio, language='ur')
```

## ğŸ“± Usage Examples

### Example 1: Real-time Detection
```bash
$ python sign_to_speech.py
ğŸš€ Loading YOLOv5 model for Pakistani Sign Language...
âœ… YOLOv5 model loaded successfully!
âœ… Loaded 20 gesture labels
ğŸ“¹ Starting camera feed...
ğŸ—£ï¸ Speaking: Detected gesture: Hello. In Urdu: Ø³Ù„Ø§Ù…. In Pashto: Ø³Ù„Ø§Ù… ÙˆØ±ÙˆØ±
```

### Example 2: Speech Recognition
```bash
$ python speech_to_sign.py  
ğŸ¤ Listening for speech...
ğŸ”¤ Recognized (Urdu): Ø³Ù„Ø§Ù…
âœ… Found partial match: 'Ø³Ù„Ø§Ù…' in 'Ø³Ù„Ø§Ù…'
ğŸ¤Ÿ GESTURE: SALAM
ğŸ´ó §ó ¢ó ¥ó ®ó §ó ¿ English: Hello
ğŸ‡µğŸ‡° Urdu: Ø³Ù„Ø§Ù…
ğŸ‡¦ğŸ‡« Pashto: Ø³Ù„Ø§Ù… ÙˆØ±ÙˆØ±
```

## ğŸ¯ Performance Tips

### For Better Detection:
- âœ… Use good lighting conditions
- âœ… Keep hand clearly visible in camera
- âœ… Make distinct gesture movements
- âœ… Hold gesture for 1-2 seconds

### For Better Speech Recognition:
- âœ… Speak clearly and slowly
- âœ… Use quiet environment
- âœ… Speak close to microphone
- âœ… Try different languages if recognition fails

## ğŸ› Troubleshooting

### Common Issues:

**"Model not found"**
```bash
# Download YOLOv5 weights
wget https://github.com/ultralytics/yolov5/releases/download/v6.0/yolov5s.pt
# Rename to best.pt or train your own model
```

**"Camera not found"** 
```bash
# Check camera permissions
# Try different camera index: VideoCapture(1)
```

**"Speech recognition failed"**
```bash
# Install pyaudio properly
pip uninstall pyaudio
pip install pyaudio
# Or use conda: conda install pyaudio
```

**"TTS not working"**
```bash
# Install espeak (Linux)
sudo apt-get install espeak espeak-data libespeak1 libespeak-dev

# Install SAPI (Windows) - usually pre-installed
```

## ğŸ”„ Updates & Improvements

### Version History:
- **v1.0**: Basic YOLOv5 detection + TTS
- **v1.1**: Added speech recognition
- **v1.2**: Pakistani language support
- **v1.3**: Improved gesture dataset

### Future Enhancements:
- [ ] More gesture classes (50+ gestures)
- [ ] Better Urdu/Pashto speech recognition
- [ ] Mobile app version  
- [ ] Real-time translation accuracy improvements
- [ ] Gesture-to-gesture translation

## ğŸ“„ License

MIT License - Feel free to use and modify for your projects!

## ğŸ¤ Contributing

1. Fork the repository
2. Add new Pakistani gestures to dataset
3. Improve speech recognition accuracy  
4. Create pull request

## ğŸ“ Support

- ğŸ“§ Email: Create an issue on GitHub
- ğŸ“– Documentation: See code comments
- ğŸ¥ Demo videos: Record your own using the scripts

## ğŸ™ Acknowledgments

- **YOLOv5** by Ultralytics for object detection
- **Pakistani Deaf Community** for gesture references
- **Google Speech Recognition** for voice processing
- **OpenCV** for computer vision
- **PyTorch** for deep learning framework

---

**ğŸ‡µğŸ‡° Made with â¤ï¸ for Pakistani Sign Language Community**

*Bridging communication barriers through AI and computer vision*